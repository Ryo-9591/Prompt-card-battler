# Ollama Configuration
# The URL where the Ollama service is running.
# If running locally without Docker, use http://localhost:11434
# If running within Docker, this might need to be http://ollama:11434 depending on your setup
OLLAMA_BASE_URL=http://localhost:11434

# The model to use for card generation.
# Examples: gemma3:1b, gemma3:270m, qwen2.5:0.5b
OLLAMA_MODEL=gemma3:1b

# Gemini API Configuration (Optional)
# Used for alternative generation methods or future features.
GEMINI_API_KEY=dummy-key-for-build
